import io
import json
import logging
from typing import Dict, Any, Optional
from datetime import datetime

from .etl_pipeline import BaseExtractor
from langchain_core.tools import ToolException

# Configure logger
logger = logging.getLogger(__name__)


class GatlingLogExtractor(BaseExtractor):
    """
    ğŸ¯ Production-ready Gatling log extractor with comprehensive validation,
    error handling, and user guidance following legacy patterns.
    """

    def __init__(self):
        super().__init__()
        logger.info("ğŸ¯ GatlingLogExtractor initialized")

    def extract(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ“¥ Extracts Gatling log file content with comprehensive validation and error handling.

        Args:
            context: Dictionary containing extraction parameters

        Returns:
            Dictionary with log content stream and metadata

        Raises:
            ToolException: With detailed user guidance for resolution
        """
        logger.info("ğŸš€ Starting Gatling log extraction process...")

        try:
            # ğŸ›¡ï¸ Step 1: Validate and extract context parameters
            validated_context = self._validate_extraction_context(context)
            api_wrapper = validated_context["api_wrapper"]
            report_id = validated_context["report_id"]

            # ğŸ“‹ Step 2: Get and validate report metadata
            report_info = self._get_and_validate_report(api_wrapper, report_id)

            # ğŸ” Step 3: Determine file paths using legacy approach
            file_paths = self._determine_file_paths(api_wrapper, report_id, report_info)

            # ğŸ“¥ Step 4: Extract log content using local file path (legacy method)
            log_content = self._extract_log_content(file_paths["test_log_path"])

            # âœ… Step 5: Prepare extraction result
            extraction_result = self._prepare_extraction_result(
                log_content,
                report_info,
                file_paths
            )

            logger.info("âœ… Gatling log extraction completed successfully")
            return extraction_result

        except ToolException:
            raise
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected extraction failure: {e}", exc_info=True)
            raise ToolException(
                f"ğŸš¨ Gatling log extraction failed unexpectedly\n"
                f"ğŸ’¥ Error: {str(e)}\n"
                f"ğŸ”§ Next Steps:\n"
                f"   1. ğŸ” Use get_report_by_id tool to verify report status\n"
                f"   2. ğŸ“‹ Check if report ID '{context.get('report_id', 'unknown')}' exists\n"
                f"   3. ğŸŒ Verify Carrier platform connectivity\n"
                f"   4. ğŸ“ Contact support if issue persists"
            )

    def _validate_extraction_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ›¡ï¸ Validates extraction context with detailed error guidance.
        """
        logger.debug("ğŸ›¡ï¸ Validating extraction context...")

        # Validate API wrapper
        api_wrapper = context.get("api_wrapper")
        if not api_wrapper:
            logger.error("âŒ Missing API wrapper in context")
            raise ToolException(
                "ğŸ”§ API wrapper not available\n"
                "ğŸ’¡ This is a system configuration issue\n"
                "ğŸ”§ Solution: Ensure Carrier API is properly initialized\n"
                "ğŸ“ Contact administrator if this persists"
            )

        # Validate report ID with flexible key names
        report_id = context.get("report_id") or context.get("source_report_id")
        if not report_id:
            logger.error("âŒ Missing report ID in context")
            available_keys = list(context.keys())
            raise ToolException(
                "ğŸ“‹ Report ID is required for extraction\n"
                f"ğŸ” Available context keys: {available_keys}\n"
                "ğŸ’¡ Solution: Provide either 'report_id' or 'source_report_id'\n"
                "ğŸ”§ Example: Use get_reports tool to find valid report IDs"
            )

        # Validate report ID format
        try:
            report_id_int = int(report_id)
            if report_id_int <= 0:
                raise ValueError("Report ID must be positive")
        except (ValueError, TypeError):
            logger.error(f"âŒ Invalid report ID format: {report_id}")
            raise ToolException(
                f"ğŸ”¢ Invalid report ID format: '{report_id}'\n"
                "âœ… Report ID must be a positive integer\n"
                "ğŸ’¡ Example: 27, 156, 2847\n"
                "ğŸ”§ Use get_reports tool to find valid report IDs"
            )

        logger.debug(f"âœ… Context validation passed for report ID: {report_id}")
        return {
            "api_wrapper": api_wrapper,
            "report_id": str(report_id_int),
            "original_context": context
        }

    def _get_and_validate_report(self, api_wrapper, report_id: str) -> Dict[str, Any]:
        """
        ğŸ“‹ Retrieves and validates report metadata with user guidance.
        """
        logger.info(f"ğŸ“‹ Fetching report metadata for ID: {report_id}")

        try:
            # Get reports list using the correct API method
            reports_data = api_wrapper.get_reports_list()

            # Handle both string and list responses
            if isinstance(reports_data, list):
                reports = reports_data
                logger.debug("ğŸ“„ Reports data already parsed as list")
            else:
                try:
                    reports = json.loads(reports_data)
                    logger.debug("ğŸ“„ Reports data parsed from JSON string")
                except json.JSONDecodeError as e:
                    logger.error(f"ğŸ’¥ Failed to parse reports JSON: {e}")
                    raise ToolException(
                        "ğŸš¨ Invalid response format from Carrier API\n"
                        "ğŸ’¥ Unable to parse reports list\n"
                        "ğŸ”§ Next Steps:\n"
                        "   1. ğŸŒ Check Carrier platform status\n"
                        "   2. ğŸ”„ Try again in a few moments\n"
                        "   3. ğŸ“ Contact support if issue persists"
                    )

            logger.debug(f"ğŸ“Š Retrieved {len(reports)} total reports")

            # Find the specific report
            report_info = None
            try:
                report_id_int = int(report_id)
                report_info = next((r for r in reports if r.get('id') == report_id_int), None)
            except (ValueError, TypeError):
                logger.error(f"âŒ Invalid report ID for comparison: {report_id}")

            if not report_info:
                logger.warning(f"ğŸ” Report ID {report_id} not found in {len(reports)} available reports")

                # Provide helpful suggestions
                available_ids = [str(r.get('id', 'unknown')) for r in reports[:5]]
                raise ToolException(
                    f"ğŸ“‹ Report not found: ID '{report_id}'\n"
                    f"ğŸ” Found {len(reports)} total reports\n"
                    f"ğŸ’¡ Recent report IDs: {', '.join(available_ids)}\n"
                    "ğŸ”§ Next Steps:\n"
                    "   1. ğŸ“‹ Use get_reports tool to see all available reports\n"
                    "   2. ğŸ” Verify the report ID is correct\n"
                    "   3. âœ… Ensure the report belongs to your project"
                )

            # Validate report has required fields
            self._validate_report_structure(report_info, report_id)

            logger.info(f"âœ… Report metadata validated for '{report_info.get('name', 'unknown')}'")
            return report_info

        except ToolException:
            raise
        except Exception as e:
            logger.error(f"ğŸ’¥ Failed to retrieve report metadata: {e}", exc_info=True)
            raise ToolException(
                f"ğŸš¨ Failed to retrieve report information\n"
                f"ğŸ’¥ Error: {str(e)}\n"
                f"ğŸ”§ Next Steps:\n"
                f"   1. ğŸŒ Check Carrier platform connectivity\n"
                f"   2. ğŸ” Verify report ID '{report_id}' exists\n"
                f"   3. ğŸ”„ Try again in a few moments"
            )

    def _validate_report_structure(self, report_info: Dict[str, Any], report_id: str):
        """
        ğŸ” Validates report structure and provides guidance for missing fields.
        """
        required_fields = ['id', 'name']
        missing_fields = [field for field in required_fields if not report_info.get(field)]

        if missing_fields:
            logger.error(f"âŒ Report structure validation failed: missing {missing_fields}")
            raise ToolException(
                f"ğŸš¨ Invalid report structure for ID '{report_id}'\n"
                f"âŒ Missing required fields: {', '.join(missing_fields)}\n"
                "ğŸ’¡ This may indicate:\n"
                "   â€¢ ğŸ”§ Report is still being processed\n"
                "   â€¢ ğŸ“‹ Report metadata is incomplete\n"
                "   â€¢ ğŸš¨ API response format changed\n"
                "ğŸ”§ Solution: Use get_report_by_id tool for detailed report status"
            )

        # Validate load generator type
        lg_type = report_info.get('lg_type', '').lower()
        if lg_type and lg_type not in ['gatling', 'jmeter']:
            logger.warning(f"âš ï¸ Unexpected load generator type: {lg_type}")

        logger.debug(f"âœ… Report structure validation passed for {required_fields}")

    def _determine_file_paths(self, api_wrapper, report_id: str, report_info: Dict[str, Any]) -> Dict[str, str]:
        """
        ğŸ” Determines file paths using legacy Carrier API approach (no download_report_as_bytes).
        """
        logger.info("ğŸ” Determining file paths using legacy API approach...")

        try:
            # Use legacy API method to get file paths
            report, test_log_file_path, errors_log_file_path = api_wrapper.get_report_file_name(report_id)

            logger.info(f"ğŸ“ File paths determined:")
            logger.info(f"   ğŸ“Š Test log: {test_log_file_path}")
            logger.info(f"   ğŸš¨ Error log: {errors_log_file_path}")

            # Validate paths exist
            if not test_log_file_path:
                logger.error("âŒ Test log file path is empty")
                raise ToolException(
                    "ğŸ“ Test log file not available\n"
                    "ğŸ” Possible reasons:\n"
                    "   â€¢ ğŸ”„ Test is still running\n"
                    "   â€¢ âŒ Test failed before generating logs\n"
                    "   â€¢ ğŸ“‹ Log files were not uploaded\n"
                    "ğŸ”§ Solution: Use get_report_by_id to check test status"
                )

            return {
                "test_log_path": test_log_file_path,
                "error_log_path": errors_log_file_path,
                "report_metadata": report
            }

        except Exception as e:
            logger.error(f"ğŸ’¥ Failed to determine file paths: {e}", exc_info=True)
            raise ToolException(
                f"ğŸš¨ Unable to locate log files for report '{report_id}'\n"
                f"ğŸ’¥ Error: {str(e)}\n"
                "ğŸ” Possible causes:\n"
                "   â€¢ ğŸ“‹ Report is still being processed\n"
                "   â€¢ ğŸ”„ Test execution is incomplete\n"
                "   â€¢ ğŸ“ Log files are not yet available\n"
                "ğŸ”§ Next Steps:\n"
                "   1. â³ Wait for test completion if still running\n"
                "   2. ğŸ” Use get_report_by_id to check status\n"
                "   3. ğŸ”„ Try again after test finishes"
            )

    def _extract_log_content(self, test_log_path: str) -> str:
        """
        ğŸ“¥ Extracts log content from local file path (legacy approach - no bytes download).
        """
        logger.info(f"ğŸ“¥ Extracting log content from: {test_log_path}")

        try:
            # Read log file content directly (legacy approach)
            # Note: In legacy code, parsers read directly from file paths
            # We simulate this by reading the file if it's accessible

            with open(test_log_path, 'r', encoding='utf-8') as log_file:
                log_content = log_file.read()

            logger.info(f"âœ… Successfully extracted {len(log_content)} characters from log file")
            logger.debug(f"ğŸ“Š Log preview (first 200 chars): {log_content[:200]}...")

            # Validate log content is not empty
            if not log_content.strip():
                logger.warning("âš ï¸ Log file appears to be empty")
                raise ToolException(
                    "ğŸ“„ Log file is empty\n"
                    "ğŸ” Possible reasons:\n"
                    "   â€¢ ğŸ”„ Test is still generating logs\n"
                    "   â€¢ âŒ Test failed immediately\n"
                    "   â€¢ ğŸ“‹ No requests were executed\n"
                    "ğŸ”§ Solution: Check test configuration and execution status"
                )

            return log_content

        except FileNotFoundError:
            logger.error(f"âŒ Log file not found: {test_log_path}")
            raise ToolException(
                f"ğŸ“ Log file not accessible\n"
                f"ğŸ“ Path: {test_log_path}\n"
                "ğŸ” This indicates:\n"
                "   â€¢ ğŸ“‹ Test is still running\n"
                "   â€¢ ğŸ”„ Log files not yet uploaded\n"
                "   â€¢ ğŸš¨ File system access issue\n"
                "ğŸ”§ Solution: Use legacy download method or wait for completion"
            )
        except PermissionError:
            logger.error(f"âŒ Permission denied accessing: {test_log_path}")
            raise ToolException(
                f"ğŸ”’ Access denied to log file\n"
                f"ğŸ“ Path: {test_log_path}\n"
                "ğŸ”§ This is a system permission issue\n"
                "ğŸ“ Contact administrator for file access resolution"
            )
        except UnicodeDecodeError as e:
            logger.error(f"âŒ Encoding error reading log file: {e}")
            raise ToolException(
                "ğŸ“„ Log file encoding issue\n"
                "ğŸ’¥ Unable to read file content as UTF-8\n"
                "ğŸ”§ Possible solutions:\n"
                "   â€¢ ğŸ“‹ File may be corrupted\n"
                "   â€¢ ğŸ”„ Try regenerating the report\n"
                "   â€¢ ğŸ“ Contact support for file analysis"
            )
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected error reading log file: {e}", exc_info=True)
            raise ToolException(
                f"ğŸš¨ Unexpected error accessing log file\n"
                f"ğŸ’¥ Error: {str(e)}\n"
                f"ğŸ“ Path: {test_log_path}\n"
                "ğŸ”§ Next Steps:\n"
                "   1. ğŸ”„ Try again in a few moments\n"
                "   2. ğŸ” Check file system status\n"
                "   3. ğŸ“ Contact support if issue persists"
            )

    def _prepare_extraction_result(self, log_content: str, report_info: Dict[str, Any],
                                   file_paths: Dict[str, str]) -> Dict[str, Any]:
        """
        âœ¨ Prepares the final extraction result with comprehensive metadata.
        """
        logger.info("âœ¨ Preparing extraction result with metadata...")

        # Create string IO stream for compatibility with legacy parsers
        log_stream = io.StringIO(log_content)

        # Prepare comprehensive result
        extraction_result = {
            "log_content_stream": log_stream,
            "log_content": log_content,  # Raw content for debugging
            "report_metadata": report_info,
            "file_paths": file_paths,
            "extraction_metadata": {
                "timestamp": datetime.now().isoformat(),
                "extractor_type": "GatlingLogExtractor",
                "content_size": len(log_content),
                "lg_type": report_info.get('lg_type', 'gatling'),
                "report_name": report_info.get('name', 'unknown'),
                "success": True
            }
        }

        # Add validation summary
        validation_summary = {
            "has_content": len(log_content.strip()) > 0,
            "content_size_bytes": len(log_content.encode('utf-8')),
            "estimated_lines": log_content.count('\n') + 1,
            "has_error_log": bool(file_paths.get("error_log_path")),
            "report_id": report_info.get('id'),
            "report_status": report_info.get('test_status', 'unknown')
        }

        extraction_result["validation_summary"] = validation_summary

        logger.info("ğŸ“Š Extraction result summary:")
        logger.info(f"   ğŸ“„ Content size: {validation_summary['content_size_bytes']} bytes")
        logger.info(f"   ğŸ“ Estimated lines: {validation_summary['estimated_lines']}")
        logger.info(f"   ğŸš¨ Has error log: {validation_summary['has_error_log']}")
        logger.info(f"   ğŸ“‹ Report status: {validation_summary['report_status']}")

        return extraction_result

    def get_missing_input_response(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ’¡ Provides guided response when required inputs are missing.
        Following legacy pattern for parameter confirmation.
        """
        logger.info("ğŸ’¡ Generating missing input guidance...")

        # Check what's missing and provide specific guidance
        missing_items = []
        available_items = []

        if not context.get("report_id") and not context.get("source_report_id"):
            missing_items.append("report_id")
        else:
            available_items.append("report_id")

        if not context.get("api_wrapper"):
            missing_items.append("api_wrapper (system configuration)")
        else:
            available_items.append("api_wrapper")

        # Get default parameters following legacy pattern
        default_parameters = self._get_default_extraction_parameters()

        return {
            "message": "ğŸ”§ Missing required inputs for Gatling log extraction",
            "status": "awaiting_input",
            "missing_inputs": missing_items,
            "available_inputs": available_items,
            "required_parameters": {
                "report_id": {
                    "description": "ğŸ“‹ The ID of the report to extract logs from",
                    "type": "integer",
                    "example": "27",
                    "required": True,
                    "how_to_get": "Use get_reports tool to find available report IDs"
                }
            },
            "optional_parameters": default_parameters,
            "next_steps": [
                "ğŸ“‹ Use get_reports tool to find available reports",
                "ğŸ” Copy the report ID you want to process",
                "ğŸš€ Call process_and_generate_report with the report ID",
                "ğŸ’¡ Default extraction parameters will be used if not specified"
            ],
            "examples": [
                {
                    "description": "ğŸ¯ Extract logs from report ID 27",
                    "command": "process_and_generate_report",
                    "parameters": {
                        "report_id": "27",
                        "pipeline_type": "gatling_to_excel"
                    }
                }
            ]
        }

    def _get_default_extraction_parameters(self) -> Dict[str, Any]:
        """
        ğŸ“‹ Returns default extraction parameters following legacy pattern.
        """
        return {
            "think_time": {
                "value": "2,0-5,0",
                "description": "ğŸ• Think time configuration for analysis",
                "type": "string"
            },
            "pct": {
                "value": "95Pct",
                "description": "ğŸ“Š Percentile for response time analysis",
                "type": "string",
                "options": ["95Pct", "99Pct", "50Pct"]
            },
            "tp_threshold": {
                "value": 10,
                "description": "ğŸ¯ Throughput threshold (requests/second)",
                "type": "integer"
            },
            "rt_threshold": {
                "value": 500,
                "description": "â±ï¸ Response time threshold (milliseconds)",
                "type": "integer"
            },
            "er_threshold": {
                "value": 5,
                "description": "ğŸš¨ Error rate threshold (percentage)",
                "type": "integer"
            }
        }

# class JMeterLogExtractor(BaseExtractor):
#     """
#     ğŸ¯ Production-ready JMeter log extractor with comprehensive validation.
#     """
#
#     def __init__(self):
#         super().__init__()
#         logger.info("âš¡ JMeterLogExtractor initialized")
#
#     def extract(self, context: Dict[str, Any]) -> Dict[str, Any]:
#         """
#         ğŸ“¥ Extracts JMeter log file content with validation and error handling.
#         """
#         logger.info("âš¡ Starting JMeter log extraction process...")
#
#         try:
#             # Reuse validation logic from Gatling extractor
#             gatling_extractor = GatlingLogExtractor()
#             validated_context = gatling_extractor._validate_extraction_context(context)
#
#             api_wrapper = validated_context["api_wrapper"]
#             report_id = validated_context["report_id"]
#
#             # Get report metadata
#             report_info = gatling_extractor._get_and_validate_report(api_wrapper, report_id)
#
#             # Validate this is a JMeter report
#             lg_type = report_info.get('lg_type', '').lower()
#             if lg_type and lg_type != 'jmeter':
#                 logger.warning(f"âš ï¸ Expected JMeter report, found: {lg_type}")
#                 raise ToolException(
#                     f"ğŸ”§ Load generator type mismatch\n"
#                     f"ğŸ“‹ Expected: JMeter\n"
#                     f"ğŸ“Š Found: {lg_type}\n"
#                     "ğŸ’¡ Solution: Use the correct pipeline type:\n"
#                     "   â€¢ âš¡ For JMeter reports: 'jmeter_to_excel
